{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "hnI_rU-WYmuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8671629c-24b4-4d74-ba0b-9fe4f49b846c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word tokenization, excluding punctuation\n",
        "def spacy_word_tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.text for token in doc if not token.is_punct]\n",
        "\n",
        "def spacy_sent_tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [sent.text for sent in doc.sents]\n",
        "\n",
        "def calculate_metrics(df):\n",
        "    # Total turn count\n",
        "    total_turn_count = len(df)\n",
        "\n",
        "    # Tokenize the text and calculate sentence count\n",
        "    df['sent_count'] = df['userInput'].apply(lambda x: len(spacy_sent_tokenize(x['text'])))\n",
        "\n",
        "    # Total sentence count\n",
        "    total_sent_count = df['sent_count'].sum()\n",
        "\n",
        "    # Mean sentence count per turn\n",
        "    mean_sent_count = round(df['sent_count'].mean(), 3)\n",
        "\n",
        "    # Standard deviation of sentence count per turn\n",
        "    std_sent_count = round(df['sent_count'].std(), 3)\n",
        "\n",
        "    # Tokenize the text and calculate word count per turn\n",
        "    df['word_count'] = df['userInput'].apply(lambda x: len(spacy_word_tokenize(x['text'])))\n",
        "    #get sum of words in the whole dataframe\n",
        "    total_word_count = df['word_count'].sum()\n",
        "    # Calculate mean word count as average number of words (tokens) per turn\n",
        "    mean_word_count = round(df['word_count'].mean(), 3)\n",
        "\n",
        "    # Calculate standard deviation of word count per turn\n",
        "    std_word_count = round(df['word_count'].std(), 3)\n",
        "\n",
        "    # Calculate vocabulary size (unique words)\n",
        "    vocab_list = []\n",
        "    for text in df['userInput']:\n",
        "        vocab_list.extend(spacy_word_tokenize(text['text']))\n",
        "    vocab_size = len(set(vocab_list))\n",
        "\n",
        "    # Calculate vocabulary size excluding stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    vocab_size_no_stopwords = len(set(vocab_list) - stop_words)\n",
        "\n",
        "    return {\n",
        "        'total_turn_count': total_turn_count,\n",
        "        'total_sent_count': total_sent_count,\n",
        "        'mean_sent_count': mean_sent_count,\n",
        "        'std_sent_count': std_sent_count,\n",
        "        'total_word_count': total_word_count,\n",
        "        'mean_word_count': mean_word_count,\n",
        "        'std_word_count': std_word_count,\n",
        "        'vocab_size': vocab_size,\n",
        "        'vocab_size_no_stopwords': vocab_size_no_stopwords\n",
        "    }\n"
      ],
      "metadata": {
        "id": "iX3lrnT0f0AA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restaurants = 'C:/Users/kleop/Documents/repos/Exercises/My_chatbot/hw1/span_extraction/restaurant8k/train_0.json'\n",
        "buses = 'C:/Users/kleop/Documents/repos/Exercises/My_chatbot/hw1/span_extraction/dstc8/Buses_1/train_0.json'\n",
        "events = 'C:/Users/kleop/Documents/repos/Exercises/My_chatbot/hw1/span_extraction/dstc8/Events_1/train_0.json'\n",
        "homes = 'C:/Users/kleop/Documents/repos/Exercises/My_chatbot/hw1/span_extraction/dstc8/Homes_1/train_0.json'\n",
        "car_rentals = 'C:/Users/kleop/Documents/repos/Exercises/My_chatbot/hw1/span_extraction/dstc8/RentalCars_1/train_0.json'\n",
        "directories = [restaurants, buses, events, homes, car_rentals]\n",
        "\n",
        "dataframes = {}\n",
        "for directory in directories:\n",
        "    if os.path.exists(directory):\n",
        "        df = pd.read_json(directory)\n",
        "\n",
        "        category = os.path.basename(os.path.dirname(directory))\n",
        "\n",
        "        dataframes[category] = df\n",
        "    else:\n",
        "        print(f\"{directory} does not exist.\")\n",
        "\n",
        "def extract_metrics(dataframes):\n",
        "    metrics = {}\n",
        "    for category, df in dataframes.items():\n",
        "        metrics[category] = calculate_metrics(df)\n",
        "    return metrics\n",
        "\n",
        "metrics = extract_metrics(dataframes)"
      ],
      "metadata": {
        "id": "Y5VrGUWYUa0V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npUnYavrCF8l",
        "outputId": "3ea9385e-3636-4bfb-82b7-1873bd95954a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'restaurant8k': {'total_turn_count': 8198,\n",
              "  'total_sent_count': 8783,\n",
              "  'mean_sent_count': 1.071,\n",
              "  'std_sent_count': 0.283,\n",
              "  'total_word_count': 62330,\n",
              "  'mean_word_count': 7.603,\n",
              "  'std_word_count': 4.738,\n",
              "  'vocab_size': 4426,\n",
              "  'vocab_size_no_stopwords': 4314},\n",
              " 'Buses_1': {'total_turn_count': 1133,\n",
              "  'total_sent_count': 1430,\n",
              "  'mean_sent_count': 1.262,\n",
              "  'std_sent_count': 0.491,\n",
              "  'total_word_count': 9694,\n",
              "  'mean_word_count': 8.556,\n",
              "  'std_word_count': 4.459,\n",
              "  'vocab_size': 501,\n",
              "  'vocab_size_no_stopwords': 429},\n",
              " 'Events_1': {'total_turn_count': 1498,\n",
              "  'total_sent_count': 1911,\n",
              "  'mean_sent_count': 1.276,\n",
              "  'std_sent_count': 0.51,\n",
              "  'total_word_count': 12209,\n",
              "  'mean_word_count': 8.15,\n",
              "  'std_word_count': 4.645,\n",
              "  'vocab_size': 773,\n",
              "  'vocab_size_no_stopwords': 692},\n",
              " 'Homes_1': {'total_turn_count': 2064,\n",
              "  'total_sent_count': 2621,\n",
              "  'mean_sent_count': 1.27,\n",
              "  'std_sent_count': 0.505,\n",
              "  'total_word_count': 16701,\n",
              "  'mean_word_count': 8.092,\n",
              "  'std_word_count': 4.362,\n",
              "  'vocab_size': 738,\n",
              "  'vocab_size_no_stopwords': 653},\n",
              " 'RentalCars_1': {'total_turn_count': 874,\n",
              "  'total_sent_count': 1103,\n",
              "  'mean_sent_count': 1.262,\n",
              "  'std_sent_count': 0.485,\n",
              "  'total_word_count': 7643,\n",
              "  'mean_word_count': 8.745,\n",
              "  'std_word_count': 5.785,\n",
              "  'vocab_size': 566,\n",
              "  'vocab_size_no_stopwords': 495}}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "for directory in directories:\n",
        "    if os.path.exists(directory):\n",
        "        df = pd.read_json(directory)\n",
        "        dfs.append(df)\n",
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "combined_metrics = calculate_metrics(combined_df)"
      ],
      "metadata": {
        "id": "TCzzPZmTQo2D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOIbC7v3XpzF",
        "outputId": "8a84999e-2a42-4ebe-ab9a-ace29f9410eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_turn_count': 13767,\n",
              " 'total_sent_count': 15848,\n",
              " 'mean_sent_count': 1.151,\n",
              " 'std_sent_count': 0.398,\n",
              " 'total_word_count': 108577,\n",
              " 'mean_word_count': 7.887,\n",
              " 'std_word_count': 4.74,\n",
              " 'vocab_size': 5202,\n",
              " 'vocab_size_no_stopwords': 5082}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_slot_info = {}\n",
        "total_unique_slots = set()\n",
        "\n",
        "for category, df in dataframes.items():\n",
        "    if 'labels' in df.columns:\n",
        "        # Extract unique slot values from the 'labels' field\n",
        "        unique_values = df['labels'].apply(lambda x: [label['slot'] for label in x if 'slot' in label] if isinstance(x, list) else []).explode()\n",
        "        unique_values = unique_values.dropna()  # Drop NaN values\n",
        "        unique_slots = unique_values.unique()\n",
        "        unique_slot_counts = len(unique_slots)\n",
        "\n",
        "        unique_slot_info[category] = {'unique_slots': unique_slots, 'count': unique_slot_counts}\n",
        "        total_unique_slots.update(unique_slots)\n",
        "\n",
        "# Print unique slots and their counts for each category\n",
        "for category, info in unique_slot_info.items():\n",
        "    print(f\"Unique slots and their counts in {category} dataset:\")\n",
        "    print(\"Unique Slots:\", info['unique_slots'])\n",
        "    print(\"Count:\", info['count'])\n",
        "\n",
        "# Total number of unique slots across all datasets\n",
        "total_unique_slots_count = len(total_unique_slots)\n",
        "print(f\"Total number of unique slots across all datasets: {total_unique_slots_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij4xXGGi6cEL",
        "outputId": "3a80bc9f-815d-471c-d1db-3d8f98679ebf"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique slots and their counts in restaurant8k dataset:\n",
            "Unique Slots: ['people' 'date' 'time' 'first_name' 'last_name']\n",
            "Count: 5\n",
            "Unique slots and their counts in Buses_1 dataset:\n",
            "Unique Slots: ['to_location' 'leaving_date' 'from_location']\n",
            "Count: 3\n",
            "Unique slots and their counts in Events_1 dataset:\n",
            "Unique Slots: ['city_of_event' 'subcategory' 'date' 'event_name']\n",
            "Count: 4\n",
            "Unique slots and their counts in Homes_1 dataset:\n",
            "Unique Slots: ['area' 'visit_date']\n",
            "Count: 2\n",
            "Unique slots and their counts in RentalCars_1 dataset:\n",
            "Unique Slots: ['dropoff_date' 'pickup_time' 'pickup_city' 'pickup_date']\n",
            "Count: 4\n",
            "Total number of unique slots across all datasets: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hFBpjuUQ_XYE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}